{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c8591e",
   "metadata": {},
   "source": [
    "# Lab Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc6b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749bc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network parameters and inputs\n",
    "\n",
    "x = np.array([1,0,1]).reshape(3,1)\n",
    "y = np.array([1]).reshape(1,1)\n",
    "lr = 0.001\n",
    "\n",
    "hwu = np.array([[0.2,-0.3], [0.4, 0.1], [-0.5, 0.2]])\n",
    "hwub = np.array([[-0.4], [0.2]])\n",
    "owu = np.array([[-0.3, -0.2]])\n",
    "owub = np.array([[0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f130675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: 0.0800\n",
      "Error: -0.9200\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# Define the ReLU activation function and derivative\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Calculate weighted sum for hidden layer\n",
    "wsh = np.dot(hwu.T, x) + hwub\n",
    "hidden = relu(wsh)\n",
    "\n",
    "# Calculate weighted sum for output layer\n",
    "wso = np.dot(owu, hidden) + owub\n",
    "pred_output = wso # No activation function on the output, as per the manual calculation\n",
    "\n",
    "# Calculate the error (MSE is commonly used, but for simplicity here we use the difference)\n",
    "error = pred_output - y\n",
    "print(f\"Predicted Output: {pred_output[0][0]:.4f}\")\n",
    "print(f\"Error: {error[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6581b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After one step of training ---\n",
      "Updated Hidden Weights (W_h):\n",
      "[[ 0.2      -0.300184]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199816]]\n",
      "\n",
      "Updated Hidden Biases (b_h):\n",
      "[[-0.4     ]\n",
      " [ 0.199816]]\n",
      "\n",
      "Updated Output Weights (W_o):\n",
      "[[-0.3      -0.199908]]\n",
      "\n",
      "Updated Output Bias (b_o):\n",
      "[[0.10092]]\n"
     ]
    }
   ],
   "source": [
    "# backward propagation\n",
    "\n",
    "# Calculate gradients for the output layer\n",
    "delta_o = error # The derivative of the loss with respect to the output is (y_hat - y)\n",
    "d_Wo = np.dot(delta_o, hidden.T)\n",
    "d_bo = delta_o\n",
    "\n",
    "# Calculate gradients for the hidden layer\n",
    "delta_h = np.dot(owu.T, delta_o) * relu_derivative(wsh)\n",
    "d_Wh = np.dot(x, delta_h.T)\n",
    "d_bh = delta_h\n",
    "\n",
    "owu -= lr * d_Wo\n",
    "owub -= lr * d_bo\n",
    "hwu -= lr * d_Wh\n",
    "hwub -= lr * d_bh\n",
    "\n",
    "print(\"\\n--- After one step of training ---\")\n",
    "print(f\"Updated Hidden Weights (W_h):\\n{hwu}\")\n",
    "print(f\"\\nUpdated Hidden Biases (b_h):\\n{hwub}\")\n",
    "print(f\"\\nUpdated Output Weights (W_o):\\n{owu}\")\n",
    "print(f\"\\nUpdated Output Bias (b_o):\\n{owub}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds413-jupyter-book-o3Nm-7iB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}