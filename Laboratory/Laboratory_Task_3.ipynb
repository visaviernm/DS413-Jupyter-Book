{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c8591e",
   "metadata": {},
   "source": [
    "# Laboratory Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335dead",
   "metadata": {},
   "source": [
    "**Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe41cc",
   "metadata": {},
   "source": [
    "**Inititalize network parameters and inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,0,1]).reshape(3,1)\n",
    "y = np.array([1]).reshape(1,1)\n",
    "lr = 0.001\n",
    "\n",
    "hwu = np.array([[0.2,-0.3], [0.4, 0.1], [-0.5, 0.2]])\n",
    "hwub = np.array([[-0.4], [0.2]])\n",
    "owu = np.array([[-0.3, -0.2]])\n",
    "owub = np.array([[0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cadc5",
   "metadata": {},
   "source": [
    "**Forward Pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: 0.0800\n",
      "Error: -0.9200\n"
     ]
    }
   ],
   "source": [
    "# Define ReLU activation function and derivative\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Calculate weighted sum for hidden layer\n",
    "wsh = np.dot(hwu.T, x) + hwub\n",
    "hidden = relu(wsh)\n",
    "\n",
    "# Calculate weighted sum for output layer\n",
    "wso = np.dot(owu, hidden) + owub\n",
    "pred_output = wso\n",
    "\n",
    "# Calculate the error\n",
    "error = pred_output - y\n",
    "print(f\"Predicted Output: {pred_output[0][0]:.4f}\")\n",
    "print(f\"Error: {error[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7c15e",
   "metadata": {},
   "source": [
    "**Backward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After one step of training ---\n",
      "Updated Hidden Weights (W_h):\n",
      "[[ 0.2      -0.300184]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199816]]\n",
      "\n",
      "Updated Hidden Biases (b_h):\n",
      "[[-0.4     ]\n",
      " [ 0.199816]]\n",
      "\n",
      "Updated Output Weights (W_o):\n",
      "[[-0.3      -0.199908]]\n",
      "\n",
      "Updated Output Bias (b_o):\n",
      "[[0.10092]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate gradients for the output layer\n",
    "delta_o = error # The derivative of the loss with respect to the output (y_hat - y)\n",
    "d_Wo = np.dot(delta_o, hidden.T)\n",
    "d_bo = delta_o\n",
    "\n",
    "# Calculate gradients for the hidden layer\n",
    "delta_h = np.dot(owu.T, delta_o) * relu_derivative(wsh)\n",
    "d_Wh = np.dot(x, delta_h.T)\n",
    "d_bh = delta_h\n",
    "\n",
    "owu -= lr * d_Wo\n",
    "owub -= lr * d_bo\n",
    "hwu -= lr * d_Wh\n",
    "hwub -= lr * d_bh\n",
    "\n",
    "print(\"\\n--- After one step of training ---\")\n",
    "print(f\"Updated Hidden Weights (W_h):\\n{hwu}\")\n",
    "print(f\"\\nUpdated Hidden Biases (b_h):\\n{hwub}\")\n",
    "print(f\"\\nUpdated Output Weights (W_o):\\n{owu}\")\n",
    "print(f\"\\nUpdated Output Bias (b_o):\\n{owub}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
